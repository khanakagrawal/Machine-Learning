# Machine-Learning
A collection of my work from the machine learning class I took Winter 2025 at UW's Paul G. Allen School of Computer Science. We learned just about all of the fundamentals: supervised learning (linear models), supervised learning (nonlinear models,parametric and nonparametric), unsupervised learning, etc.

Here are the concepts I learned:
1. Linear/Polynomial regression
2. Regularization: LASSO (L1) and Ridge (L2)
3. Logistic/Binary regression (sigmoid)
4. Multi-class classification (softmax)
5. Loss functions (mean squared error, cross entropy)
6. Optimization (gradient descent - normal, stochastic, minibatch)
7. Validation and hyperparameter search (Leave one out, k-fold cross validation)
8. Preprocessing (demeaning, normalizing, standardizing)
9. Neural Networks (activations - ReLU, sigmoid, softmax, Tanh)
10. Kernel trick (Polynomial (including/upto degree k), Radial Basis Function Kernel, Sigmoid)
11. Bootstrapping
12. Decision Trees
13. Random Forest, Bagging, Boosting, Gradient Boosting
14. K-means and smart initialization
15. Convolution Neural Network
16. Matrix Completion
17. Singular Value Decomposition (SVD) & Principal Component Analysis (PCA)
18. Gaussian Mixture Models (GMM)

